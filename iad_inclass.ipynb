{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pylab as plt\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "\n",
    "from sklearn.cross_validation import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file='submission.csv',\n",
    "                             target='Prediction', index_label=\"Id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(0, predicted_labels.shape[0]),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of data: train_data - (27595, 20), test_data - (13593, 20), train_target - (27595, 1)\n",
      "Proportion train/test:  2\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"train_data.csv\")\n",
    "test_data = pd.read_csv(\"test_data.csv\").drop(\"Unnamed: 0\", axis=1)\n",
    "train_target = pd.read_csv(\"train_target.csv\")\n",
    "print(\"Shapes of data: train_data - {}, test_data - {}, train_target - {}\".format(train_data.shape, test_data.shape, train_target.shape))\n",
    "print(\"Proportion train/test: \", int(train_data.shape[0]/test_data.shape[0]))\n",
    "\n",
    "seed = 4767"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fad4c54bcf8>]], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEPCAYAAAC+35gCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHNJJREFUeJzt3X9wVNUd9/HP7sY0s8mSdUNSwMhICVRdGKCEEbAGBFpF\n6JSZzmTKQ2uXQhVGB9xR+whYHRSkjGAIBGgVrVDbceJosP2jOh1ISMVog5gRFqmuASpIhOySmGjA\n/DjPHzzsGsHmmF+76Ps145B7957d7/mO7Cdn792LwxhjBACABWeiCwAAXD4IDQCANUIDAGCN0AAA\nWCM0AADWCA0AgDVCAwBgjdAAJP3oRz/Sr3/960SX0cnq1as1bNiwRJcBdEJoAL2ora2t157LGCOH\nw9Frzwf0BkID33rz58/Xrl27tH37djmdTrlcLlVWVurBBx/U9ddfr/T0dA0dOlSLFy/WJ598Ehu3\nfft2XXHFFaqoqNAPfvADpaWladeuXZKkDRs26Oqrr1ZGRoZmz56tv/71r3I6nfroo49i49966y3d\ncsst8ng8ysnJ0c9+9jP997//jT33Qw89pGPHjsVqeuSRR/q3McClGOBbrrGx0RQUFJif//zn5tSp\nU+bjjz82n3/+uVm9erXZu3evOXbsmNm9e7e57rrrTCAQiI179tlnjdPpNDfccIOpqKgwR44cMfX1\n9ebFF180KSkpZtOmTSYcDpvt27ebIUOGGKfTaU6cOGGMMSYUCpmMjAyzcuVK895775mDBw+awsJC\nM3LkSHPu3DnT0tJiHnjgATN06NBYTZ9++mmiWgTEEBqAMWbGjBlm/vz5//OYsrIyk5aWFtu+EBp7\n9+7tdNyNN95obr/99k77HnjggU6hEQgEzNy5czsdc/bsWeN2u83LL79sjDFm1apVZtiwYd2eE9AX\nUhK90gGS1UsvvaTi4mKFw2F98skn6ujo0Oeff666ujoNGjQodlx+fn6ncYcOHdK8efM67Zs0aVKn\n7erqan3wwQfyeDyd9p87d07vv/9+L88E6D2EBnAJb775pgoLC7VixQqtW7dOV155paqqqhQIBPT5\n55/HjnO5XEpNTb1ofFcnsDs6OvTLX/5Sy5Ytk/nSjaazsrJ6ZxJAHyA0AEmpqalqb2+Pbe/du1fZ\n2dlauXJlbF9paanVc11//fWqqqrSokWLYvuqqqo6HZOfn6933nnnf15S++WagGTA1VOApGHDhumt\nt95SbW2tIpGIRo4cqdOnT+uZZ57RkSNHtGPHDm3dutXque699149//zzKikp0QcffKAdO3boz3/+\ns6T4CmT58uV699139Ytf/ELV1dU6evSoysvLdc899+jo0aOxmurq6vTGG28oEomopaWlT+YOfC2J\nPqkCJIPa2lozZcoUk5GRYZxOp9mzZ4956KGHzKBBg0xGRoaZNWuWef75543T6TTHjh0zxpw/EX7F\nFVdc8vk2bNhgcnNzjdvtNrfeeqt58sknjdPpNNFoNHbMwYMHzZw5c4zP5zNut9uMGDHC3HnnnebM\nmTPGGGNaW1vNvHnzjM/nM06n06xcubLvGwF0wWHM//6X+yKRiEpKStTY2CiHw6EZM2Zo5syZeuGF\nF7Rr1y5lZmZKkubOnauxY8dKksrKylReXi6Xy6VAIKAxY8ZIkmpra7Vlyxa1trZq3LhxCgQCks5/\nIaqkpES1tbXyeDwKBoMaOHBgH0Yl0L8eeeQRlZSU6NSpU4kuBeiZrlLlzJkz5siRI8YYY1paWsyS\nJUvM8ePHTWlpqfn73/9+0fEffvihuf/++01bW5v5+OOPzd133206OjqMMcYsW7bMvP/++8YYYx57\n7DHz9ttvG2OMefXVV81TTz1ljDFm7969pqioyCrxDh48aHXctwG9iEt0L1pbW83vf/97884775gP\nPvjAPPXUU8bj8Zjly5f3ey2J7kUyoRdxPelFl+c0vF6vrrnmGklSWlqarrrqKkWj0QuBc9Hx+/bt\n0+TJk+VyuZSTk6PBgwcrHA6roaFBLS0tysvLkyQVFBSourpa0vnLD6dMmSJJmjhxog4cOGAVeKFQ\nyOq4bwN6EZfoXjgcDlVUVGjGjBkaNWqUioqK9OCDD+rRRx/t91oS3YtkQi/ietKLr3X11KlTp3Ts\n2DGNGDFChw8f1iuvvKLKykoNHz5ct99+u9xut6LRqEaOHBkb4/P5FI1G5XK5Ol1KmJWVFQufaDQa\ne8zpdCo9PV3Nzc3KyMjo9sSARHG5XPrHP/6R6DKAPmF99dTZs2f1xBNPKBAIKC0tTbfccotKSkr0\n+OOPy+v1aseOHb1W1KVWMACAxLNaabS3t2v9+vUqKCjQhAkTJEkDBgyIPT59+nStXbtW0vmVRX19\nfeyxSCQin88nn8+nSCRy0f4LYy5sd3R0qKWl5ZKrjFAo1GlZVVhY+HXm+o1GL+LoRRy9iKMXcYWF\nhZ2+d+T3++X3+63GWoXG1q1blZubq9tuuy22r6GhQV6vV9L5b89effXVks5/aWnjxo2aPXu2otGo\n6urqlJeXJ4fDIbfbrXA4rOHDh6uyslIzZ86MjdmzZ49GjBihqqoqjRo16pJ1XGpiX7xr6LeZx+NR\nU1NTostICvQijl7E0Yu4IUOGdDtEu7zk9vDhw3r44Yc1dOhQORwOORwOzZ07V6+99pqOHj0qh8Oh\n7Oxs3XHHHbEQKSsr0+7du5WSknLRJbebN2+OXXI7f/58SVJra6s2bdqko0ePyuPxaOnSpcrJybGa\nAKFxHn8h4uhFHL2IoxdxQ4YM6fbYLkMj2REa5/EXIo5exNGLOHoR15PQuOzvPeWs2pXY1x/hV9vA\nQV0fCADfAJd9aLQ+U5zQ10+9b7VEaAD4luCGhQAAa4QGAMAaoQEAsEZoAACsERoAAGuEBgDAGqEB\nALBGaAAArBEaAABrhAYAwBqhAQCwRmgAAKwRGgAAa4QGAMAaoQEAsEZoAACsERoAAGuEBgDAGqEB\nALBGaAAArBEaAABrhAYAwBqhAQCwRmgAAKwRGgAAa4QGAMAaoQEAsEZoAACsERoAAGuEBgDAGqEB\nALBGaAAArBEaAABrhAYAwFpKVwdEIhGVlJSosbFRDodD06dP12233abm5mZt2LBBp0+fVk5OjoLB\noNxutySprKxM5eXlcrlcCgQCGjNmjCSptrZWW7ZsUWtrq8aNG6dAICBJamtrU0lJiWpra+XxeBQM\nBjVw4MC+mzUAoFu6XGm4XC796le/0hNPPKHVq1fr1Vdf1YkTJ7Rz506NHj1axcXF8vv9KisrkyQd\nP35cVVVVKioq0rJly7Rt2zYZYyRJ27Zt06JFi1RcXKyTJ0+qpqZGkrR7925lZGRo48aNmjVrlp57\n7rk+nDIAoLu6DA2v16trrrlGkpSWlqarrrpKkUhE+/bt05QpUyRJU6dOVXV1tSRp3759mjx5slwu\nl3JycjR48GCFw2E1NDSopaVFeXl5kqSCgoLYmOrq6thzTZw4UQcOHOj1iQIAeu5rndM4deqUjh07\nppEjR6qxsVFer1fS+WBpbGyUJEWj0U4fLfl8PkWjUUWjUWVlZcX2Z2VlKRqNxsZceMzpdCo9PV3N\nzc09mxkAoNd1eU7jgrNnz+qJJ55QIBBQWlraRY87HI5eK+rCx1lfFgqFFAqFYtuFhYW99prd5XI5\n5fZ4El2GUlNT5UmCOpIBvYijF3H0orPS0tLYz36/X36/32qcVWi0t7dr/fr1Kigo0IQJEySdX100\nNDTE/szMzJR0fmVRX18fGxuJROTz+eTz+RSJRC7af2HMhe2Ojg61tLQoIyPjojq+zsT6S3t7h5qa\nmhJdhjweT1LUkQzoRRy9iKMXcR6Pp9u/dFt9PLV161bl5ubqtttui+0bP368KioqJEkVFRXKz8+X\nJOXn5+v1119XW1ubTp06pbq6OuXl5cnr9crtdiscDssYo8rKylgA5efna8+ePZKkqqoqjRo1qluT\nAQD0rS5XGocPH9a//vUvDR06VL/97W/lcDg0d+5czZkzR0VFRSovL1d2draCwaAkKTc3V5MmTVIw\nGFRKSooWLlwY++hqwYIF2rx5c+yS27Fjx0qSpk2bpk2bNmnJkiXyeDxaunRpH04ZANBdDvNVJxAu\nEx/Oyk/o66fet1rt3x+d0Boklt5fRC/i6EUcvYgbMmRIt8fyjXAAgDVCAwBgjdAAAFgjNAAA1ggN\nAIA1QgMAYI3QAABYIzQAANYIDQCANUIDAGCN0AAAWCM0AADWCA0AgDVCAwBgjdAAAFgjNAAA1ggN\nAIA1QgMAYI3QAABYIzQAANYIDQCANUIDAGCN0AAAWCM0AADWCA0AgDVCAwBgjdAAAFgjNAAA1ggN\nAIA1QgMAYI3QAABYIzQAANYIDQCANUIDAGCN0AAAWEvp6oCtW7dq//79yszM1Lp16yRJL7zwgnbt\n2qXMzExJ0ty5czV27FhJUllZmcrLy+VyuRQIBDRmzBhJUm1trbZs2aLW1laNGzdOgUBAktTW1qaS\nkhLV1tbK4/EoGAxq4MCBfTFXAEAPdbnSuPnmm7VixYqL9s+ePVtr167V2rVrY4Fx/PhxVVVVqaio\nSMuWLdO2bdtkjJEkbdu2TYsWLVJxcbFOnjypmpoaSdLu3buVkZGhjRs3atasWXruued6c34AgF7U\nZWhce+21Sk9Pv2j/hTD4on379mny5MlyuVzKycnR4MGDFQ6H1dDQoJaWFuXl5UmSCgoKVF1dLUmq\nrq7WlClTJEkTJ07UgQMHejQhAEDf6fLjqa/yyiuvqLKyUsOHD9ftt98ut9utaDSqkSNHxo7x+XyK\nRqNyuVzKysqK7c/KylI0GpUkRaPR2GNOp1Pp6elqbm5WRkZGd0sDAPSRbp0Iv+WWW1RSUqLHH39c\nXq9XO3bs6LWCLrWCAQAkh26tNAYMGBD7efr06Vq7dq2k8yuL+vr62GORSEQ+n08+n0+RSOSi/RfG\nXNju6OhQS0vLV64yQqGQQqFQbLuwsLA75fcql8spt8eT6DKUmpoqTxLUkQzoRRy9iKMXnZWWlsZ+\n9vv98vv9VuOsQsMY02kF0NDQIK/XK0l68803dfXVV0uS8vPztXHjRs2ePVvRaFR1dXXKy8uTw+GQ\n2+1WOBzW8OHDVVlZqZkzZ8bG7NmzRyNGjFBVVZVGjRr1lXV8nYn1l/b2DjU1NSW6DHk8nqSoIxnQ\nizh6EUcv4jweT7d/6e4yNIqLi3Xo0CE1NTVp8eLFKiwsVCgU0tGjR+VwOJSdna077rhDkpSbm6tJ\nkyYpGAwqJSVFCxculMPhkCQtWLBAmzdvjl1ye+GKq2nTpmnTpk1asmSJPB6Pli5d2q2JAAD6nsNc\n5icRPpyVn9DXT71vtdq/PzqhNUj8FvVF9CKOXsTRi7ghQ4Z0eyzfCAcAWCM0AADWCA0AgDVCAwBg\njdAAAFgjNAAA1ggNAIA1QgMAYI3QAABYIzQAANYIDQCANUIDAGCN0AAAWCM0AADWCA0AgDVCAwBg\njdAAAFgjNAAA1ggNAIA1QgMAYI3QAABYIzQAANYIDQCANUIDAGCN0AAAWCM0AADWCA0AgDVCAwBg\njdAAAFgjNAAA1ggNAIA1QgMAYI3QAABYIzQAANYIDQCANUIDAGAtpasDtm7dqv379yszM1Pr1q2T\nJDU3N2vDhg06ffq0cnJyFAwG5Xa7JUllZWUqLy+Xy+VSIBDQmDFjJEm1tbXasmWLWltbNW7cOAUC\nAUlSW1ubSkpKVFtbK4/Ho2AwqIEDB/bRdAEAPdHlSuPmm2/WihUrOu3buXOnRo8ereLiYvn9fpWV\nlUmSjh8/rqqqKhUVFWnZsmXatm2bjDGSpG3btmnRokUqLi7WyZMnVVNTI0navXu3MjIytHHjRs2a\nNUvPPfdcb88RANBLugyNa6+9Vunp6Z327du3T1OmTJEkTZ06VdXV1bH9kydPlsvlUk5OjgYPHqxw\nOKyGhga1tLQoLy9PklRQUBAbU11dHXuuiRMn6sCBA703OwBAr+rWOY3GxkZ5vV5JktfrVWNjoyQp\nGo12+mjJ5/MpGo0qGo0qKysrtj8rK0vRaDQ25sJjTqdT6enpam5u7t5sAAB9qldOhDscjt54GkmK\nfZwFAEg+XZ4IvxSv16uGhobYn5mZmZLOryzq6+tjx0UiEfl8Pvl8PkUikYv2XxhzYbujo0MtLS3K\nyMi45OuGQiGFQqHYdmFhYXfK71Uul1NujyfRZSg1NVWeJKgjGdCLOHoRRy86Ky0tjf3s9/vl9/ut\nxlmFhjGm0wpg/Pjxqqio0Jw5c1RRUaH8/HxJUn5+vjZu3KjZs2crGo2qrq5OeXl5cjgccrvdCofD\nGj58uCorKzVz5szYmD179mjEiBGqqqrSqFGjvrKOrzOx/tLe3qGmpqZElyGPx5MUdSQDehFHL+Lo\nRZzH4+n2L91dhkZxcbEOHTqkpqYmLV68WIWFhZozZ46KiopUXl6u7OxsBYNBSVJubq4mTZqkYDCo\nlJQULVy4MPbR1YIFC7R58+bYJbdjx46VJE2bNk2bNm3SkiVL5PF4tHTp0m5NBADQ9xzmMj+J8OGs\n/IS+fup9q9X+/dEJrUHit6gvohdx9CKOXsQNGTKk22P5RjgAwBqhAQCwRmgAAKwRGgAAa4QGAMAa\noQEAsEZoAACsERoAAGuEBgDAGqEBALBGaAAArBEaAABrhAYAwBqhAQCwRmgAAKwRGgAAa4QGAMAa\noQEAsEZoAACsERoAAGuEBgDAGqEBALBGaAAArBEaAABrhAYAwBqhAQCwRmgAAKwRGgAAa4QGAMAa\noQEAsEZoAACsERoAAGuEBgDAGqEBALBGaAAArBEaAABrKT0ZfNddd8ntdsvhcMjlcmnNmjVqbm7W\nhg0bdPr0aeXk5CgYDMrtdkuSysrKVF5eLpfLpUAgoDFjxkiSamtrtWXLFrW2tmrcuHEKBAI9nhgA\noPf1KDQcDocefvhhZWRkxPbt3LlTo0eP1k9/+lPt3LlTZWVlmjdvno4fP66qqioVFRUpEono0Ucf\n1caNG+VwOLRt2zYtWrRIeXl5WrNmjWpqajR27NgeTw4A0Lt69PGUMUbGmE779u3bpylTpkiSpk6d\nqurq6tj+yZMny+VyKScnR4MHD1Y4HFZDQ4NaWlqUl5cnSSooKIiNAQAklx6vNFatWiWn06kZM2Zo\n+vTpamxslNfrlSR5vV41NjZKkqLRqEaOHBkb6/P5FI1G5XK5lJWVFduflZWlaDTak7IAAH2kR6Hx\n6KOP6sorr9Qnn3yiVatWaciQIRcd43A4evISnYRCIYVCodh2YWFhrz13d7lcTrk9nkSXodTUVHmS\noI5kQC/i6EUcveistLQ09rPf75ff77ca16PQuPLKKyVJAwYM0IQJExQOh+X1etXQ0BD7MzMzU9L5\nlUV9fX1sbCQSkc/nk8/nUyQSuWj/pXydifWX9vYONTU1JboMeTyepKgjGdCLOHoRRy/iPB5Pt3/p\n7vY5jXPnzuns2bOSpLNnz+qdd97R0KFDNX78eFVUVEiSKioqlJ+fL0nKz8/X66+/rra2Np06dUp1\ndXXKy8uT1+uV2+1WOByWMUaVlZWaMGFCd8sCAPShbq80Ghsb9fjjj8vhcKi9vV033XSTxowZo+HD\nh6uoqEjl5eXKzs5WMBiUJOXm5mrSpEkKBoNKSUnRwoULYx9dLViwQJs3b45dcsuVUwCQnBzmy5c/\nXWY+nJWf0NdPvW+12r8/OqE1SCy9v4hexNGLOHoRd6nzz7b4RjgAwBqhAQCwRmgAAKwRGgAAa4QG\nAMAaoQEAsEZoAACsERoAAGuEBgDAGqEBALBGaAAArBEaAABrPfr3NAAA9lxn6qXo6USXIfXghoWE\nBgD0l+hpff77/5voKqSb9nV7KB9PAQCsERoAAGuEBgDAGqEBALBGaAAArBEaAABrhAYAwBqhAQCw\nRmgAAKwRGgAAa4QGAMAaoQEAsEZoAACsERoAAGuEBgDAGqEBALBGaAAArBEaAABrhAYAwBqhAQCw\nRmgAAKylJLqAC2pqavTss8/KGKObb75Zc+bMSXRJAIAvSYqVRkdHh55++mmtWLFC69ev1969e3Xi\nxIlElwUA+JKkCI1wOKzBgwcrOztbKSkpuvHGG1VdXZ3osgAAX5IUoRGNRpWVlRXb9vl8ikajCawI\nAHApSXNOo7uu+D93JvT1Hb6BCX19AOhPSREaPp9P9fX1se1oNCqfz3fRcaFQSKFQKLZdWFioQfN+\n0y81Xg48Hk+iS0ga9CKOXsQlvBdDhkg37UtsDf9faWlp7Ge/3y+/3281Lik+nsrLy1NdXZ1Onz6t\ntrY27d27V/n5+Rcd5/f7VVhYGPvvi5P+tqMXcfQijl7E0Yu40tLSTu+ltoEhJclKw+l0asGCBVq1\napWMMZo2bZpyc3MTXRYA4EuSIjQkaezYsSouLk50GQCA/yEpPp7qrq+zpPqmoxdx9CKOXsTRi7ie\n9MJhjDG9WAsA4Bvssl5pAAD6F6EBALCWNCfC/xebmxk+88wzqqmp0Xe+8x3ddddduuaaa/q/0H7Q\nVS9ee+01vfzyy5KktLQ0/eY3v9HQoUMTUWqfs73JZTgc1u9+9zvdc889uuGGG/q5yv5h04tQKKTt\n27ervb1dAwYM0MMPP5yASvteV7347LPPtGnTJtXX16ujo0M/+clPNHXq1MQU24e2bt2q/fv3KzMz\nU+vWrbvkMd163zRJrr293dx9993m1KlTprW11dx3333m+PHjnY7Zv3+/eeyxx4wxxrz33ntm+fLl\niSi1z9n04j//+Y/59NNPjTHGvP3229/qXlw4buXKlWbNmjXmjTfeSEClfc+mF59++qkJBoMmEokY\nY4xpbGxMRKl9zqYXL730kvnLX/5ijDnfh/nz55u2trZElNun3n33XXPkyBFz7733XvLx7r5vJv3H\nUzY3M6yurtaUKVMkSSNGjNBnn32mhoaGRJTbp2x6MXLkSLndbknne/FNvYeX7U0uX3nlFU2cOFED\nBgxIQJX9w6YXr732mm644YbYnRa+qf2w6YXD4VBLS4sk6ezZs/J4PHK5XIkot09de+21Sk9P/8rH\nu/u+mfShYXMzw2/LDQ+/7jx37dqlsWPH9kdp/c72/4vq6mr9+Mc/7u/y+pVNLz766CM1Nzdr5cqV\nWrZsmSorK/u7zH5h04tbb71Vx48f15133qn7779fgUCgn6tMDt1930z60ED3HDx4UBUVFZo3b16i\nS0mYZ599ttP8zbf46vKOjg4dOXJEy5Yt0/Lly/Xiiy+qrq4u0WUlRE1NjYYNG6Y//vGPWrt2rZ5+\n+mmdPXs20WVdNpL+RLjNzQx9Pp8ikUhsOxKJXPKGh5c72xs7Hjt2TE8++aSWL1+ujIyM/iyx39j0\nora2Vhs2bJAxRk1NTXr77beVkpJyyfuaXc5s/454PB6lpqYqNTVV1113nY4ePapBgwb1d7l9yqYX\nFRUVsZPjgwYNUk5Ojk6cOKHhw4f3a62J1t33zaRfadjczDA/P1979uyRJL333ntKT0+X1+tNRLl9\nyqYX9fX1Wr9+ve6+++5v3BvCF9n0oqSkRCUlJdq8ebMmTpyohQsXfuMCQ7LrxYQJE3T48GF1dHTo\n3Llzev/997+R93ez6cXAgQN14MABSVJDQ4NOnjyp7373u4kot88ZY75yhd3d983L4hvhNTU1+tOf\n/hS7meGcOXP0z3/+Uw6HQzNmzJAkPf3006qpqVFaWpoWL16s733vewmuum901Ys//OEP+ve//63s\n7GwZY+RyubRmzZpEl90nbP6/uGDLli0aP378N/qS26568be//U0VFRVyOp2aPn26Zs6cmeCq+0ZX\nvThz5oy2bNmiM2fOSJLmzJmjH/7whwmuuvcVFxfr0KFDampqUmZmpgoLC9XW1tbj983LIjQAAMkh\n6T+eAgAkD0IDAGCN0AAAWCM0AADWCA0AgDVCAwBgjdAAAFgjNAAA1v4fX7IAP6scXVkAAAAASUVO\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fad4c54b748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_target.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Undersampling\n",
    "\n",
    "* по картинке выше понятно, что выборка несбалансированна, поэтому были предприняты попытки произвести undersample превуалирующего класс \"0\"\n",
    "* как вручную, так и средствами sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def man_undersample(data, target):\n",
    "    \n",
    "    yes_indexes = target[target[\"1\"]==1].index\n",
    "    no_indexes = target[target[\"1\"]==0].index\n",
    "    new_train_yes = data.iloc[yes_indexes]\n",
    "    new_train_no = data.iloc[no_indexes][:int(1.5*len(new_train_yes))]\n",
    "    us_train = pd.concat([new_train_yes, new_train_no], axis=0).reset_index(drop=True)\n",
    "\n",
    "    new_train_target_yes = target.iloc[new_train_yes.index]\n",
    "    new_train_target_no = target.iloc[new_train_no.index]\n",
    "    us_target = pd.concat([new_train_target_yes, new_train_target_no], axis=0).reset_index(drop=True)\n",
    "    us_target.columns=[\"target\"]\n",
    "    \n",
    "    return us_train, us_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def undersample(data, target):\n",
    "    random_us = RandomUnderSampler(random_state=seed)\n",
    "    data_, target_ = random_us.fit_sample(data, target['target'].values.ravel())\n",
    "    return pd.DataFrame(data_, columns=data.columns), pd.DataFrame(target_, columns=target.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import get_dummies\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_target(target):\n",
    "    return np.array(train_target).reshape(train_target.shape[0],)\n",
    "\n",
    "def preprocess(data):\n",
    "    \n",
    "    for col_to in ['housing', 'default', 'loan'] :\n",
    "        data[col_to] = data[col_to].map(lambda x: 1 if x == 'yes' else 0)\n",
    "\n",
    "    categorical_cols = [\"job\", \"marital\", \"education\", \"contact\", \"month\", \"day_of_week\", \"poutcome\"]\n",
    "    cat_data = pd.get_dummies(data[categorical_cols])\n",
    "    \n",
    "    num_data = data.drop(categorical_cols, axis=1)\n",
    "    num_data = pd.DataFrame(MinMaxScaler().fit_transform(num_data), columns = num_data.columns)\n",
    "    \n",
    "    return pd.concat([cat_data, num_data], axis=1).drop(['previous'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN meta-features\n",
    "\n",
    "* небольшая функция для кластеризации объектов. \n",
    "* метки классов, присвоенных kNN-ом, могут быть использованы в обучении\n",
    "* положительных результатов не показало"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_kMeans_features(data, k_range):\n",
    "    kMeans_meta_features = pd.DataFrame()\n",
    "\n",
    "    for i in k_range:\n",
    "        clr = KMeans(n_clusters=i, verbose=100, n_jobs=2)\n",
    "        clr.fit(data)\n",
    "        kMeans_meta_features[str(i)+\"Means\"] = [str(i) for i in clr.labels_]\n",
    "    return pd.get_dummies(kMeans_meta_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE\n",
    "\n",
    "* была идея, схожая с кластеризацией, только учитывающая непосредственно \"расстояние\" между объектами\n",
    "* функция get_tsne возвращает np.array train_tsne, который состоит из координат $(x,y)$ в $R^2$ каждого объекта\n",
    "* если бы картинка ниже имела более выраженные кластеры, то можно было бы использовать эти координаты в обучении\n",
    "---\n",
    "* использую именно пакет MulticoreTSNE, т.к. sklearn реализация медленнее и вообще не работает при большом количестве объектов (лично у меня, при больше 40 000 сэмплов)\n",
    "* не уверен, что этот пакет установлен, поэтому можно просто посмотреть на картинку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from MulticoreTSNE import MulticoreTSNE as TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tsne(data, labels):\n",
    "    tsne = TSNE(n_jobs=4)\n",
    "    train_tsne = tsne.fit_transform(np.array(data))\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.scatter(train_tsne[:,0], train_tsne[:,1], c = np.array(labels))\n",
    "    ax = plt.axis('off')\n",
    "    return train_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get_tsne(np.array(scaled_train), train_target.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FS with RF\n",
    "\n",
    "* функция предназначена для Feature Selection\n",
    "* возвращает датасет с отобранными select_model классификатором признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features_rf(data, target, select_model):\n",
    "    for_seleсt = select_model.fit(data, target)\n",
    "    model = SelectFromModel(for_seleсt, prefit=True)\n",
    "    selected_train = model.transform(data)\n",
    "    print(\"Old shape: {}, new shape: {}\".format(data.shape, selected_train.shape))\n",
    "    return selected_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature hashing\n",
    "\n",
    "* положительных результатов не дало\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from scipy.sparse import csc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_hash(data):\n",
    "    raw_X_train = [dict(row[1]) for row in data.iterrows()]\n",
    "    fh = FeatureHasher(n_features = 2 ** 20)\n",
    "    return fh.transform(raw_X_train)\n",
    "\n",
    "def sparse_to_df(sp_mx):\n",
    "\n",
    "    fh_df = sp_mx.tocoo(copy=False)\n",
    "\n",
    "    return pd.DataFrame({'index': fh_df.row, 'col': fh_df.col, 'data': fh_df.data}\n",
    "                 )[['index', 'col', 'data']].sort_values(['index', 'col']\n",
    "                 ).reset_index(drop=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaled_train = preprocess(train_data)\n",
    "test_data = preprocess(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgb\n",
    "\n",
    "* xgboost был настроен согласно следующему [туториалу](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n",
    "* классификатора лучше, увы, найти не удалось и лучший сабмит в итоге был получен из fine tuned xgb\n",
    "* модель в функции xgb_best_params именно она\n",
    "* попытки построения ансамбля также не увенчались успехом\n",
    "---\n",
    "\n",
    "#### Финальный сабмит получен этой моделью!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xgb_best_params(data, target):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2,\n",
    "                                                        random_state=442, stratify=target)\n",
    "    \n",
    "    \n",
    "    gbm = xgb.XGBClassifier(max_depth=3, learning_rate=0.02, n_estimators=1200,\n",
    "                                            min_child_weight=1, gamma=0.0,subsample=0.8,\n",
    "                                            colsample_bytree=0.8, reg_alpha=0.01, reg_lambda=0.05,\n",
    "                                            scale_pos_weight=1, max_delta_step=9, nthread=15)\n",
    "    \n",
    "    gbm.fit(X_train, y_train, eval_metric ='auc')\n",
    "    \n",
    "    predictions = gbm.predict_proba(X_test)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, predictions[:, 1], pos_label=1)\n",
    "    print(\"AUC-ROC on test: %.4g\" % auc(fpr, tpr))\n",
    "    \n",
    "    skf = StratifiedKFold(target, n_folds=5, random_state=seed)\n",
    "    results = cross_val_score(gbm, data, target, cv=skf, scoring='roc_auc')\n",
    "    for ind, i in enumerate(results):\n",
    "        print(\"fold #{:d}: {:f}\".format(ind, i))\n",
    "        \n",
    "    print(\"mean AUC-ROC on cv : %.4f%% (%.4f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xgb_grid_search(data, target, skf=True):\n",
    "    \n",
    "    if skf:\n",
    "        skf = StratifiedKFold(np.array(target), n_folds=5, random_state=seed)\n",
    "        for train_index, test_index in skf:\n",
    "            X_train, X_test = data[train_index], data[test_index]\n",
    "            y_train, y_test = target[train_index], target[test_index]\n",
    "    else: \n",
    "        X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=seed)\n",
    "    \n",
    "    gbm_params = {   \n",
    "        \"n_estimators\":[300, 500, 1000]\n",
    "        \n",
    "    }\n",
    "    print(\"GridSeachCV proceeding...\")\n",
    "    \n",
    "    \n",
    "    gbm_gs = GridSearchCV(xgb.XGBClassifier(max_depth=3, learning_rate=0.02, n_estimators=1200,\n",
    "                                            min_child_weight=1, gamma=0.0,subsample=0.8,\n",
    "                                            colsample_bytree=0.8, reg_alpha=0.01, reg_lambda=0.05,\n",
    "                                            scale_pos_weight=1, max_delta_step=9, nthread=15), \n",
    "                          gbm_params, n_jobs=15, cv=5, verbose=2)\n",
    "    \n",
    "    gbm_gs.fit(X_train, y_train)\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    \"\"\"skf = StratifiedKFold(target, n_folds=5, random_state=seed)\n",
    "    results = cross_val_score(gbm_gs.best_estimator_, data, target, cv=skf, scoring='roc_auc')\n",
    "    for ind, i in enumerate(results):\n",
    "        print(\"fold #{:d}: {:f}\".format(ind, i))\"\"\"\n",
    "    \n",
    "    for params, mean_score, scores in gbm_gs.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() / 2, params))\n",
    "        \n",
    "    print(101*\"=\"+ \"\\nBEST PARAMETERS: \", gbm_gs.best_params_, \"\\n\"+101*\"=\"+\"\\n\")\n",
    "    \n",
    "    predictions = gbm_gs.best_estimator_.predict_proba(X_test)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, predictions[:, 1], pos_label=1)\n",
    "    print(\"AUC : %.4g\" % auc(fpr, tpr))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rudolph/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old shape: (27595, 56), new shape: (27595, 10)\n"
     ]
    }
   ],
   "source": [
    "fs_train = get_features_rf(scaled_train, train_target, RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC on test: 0.9506\n",
      "fold #0: 0.951469\n",
      "fold #1: 0.946904\n",
      "fold #2: 0.941410\n",
      "fold #3: 0.946638\n",
      "fold #4: 0.953145\n",
      "mean AUC-ROC on cv : 94.7913% (0.4122%)\n"
     ]
    }
   ],
   "source": [
    "xgb_best_params(np.array(fs_train), np.array(train_target).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC on test: 0.951\n",
      "fold #0: 0.952021\n",
      "fold #1: 0.947516\n",
      "fold #2: 0.942340\n",
      "fold #3: 0.947094\n",
      "fold #4: 0.954434\n",
      "mean AUC-ROC on cv : 94.8681% (0.4203%)\n"
     ]
    }
   ],
   "source": [
    "xgb_best_params(np.array(scaled_train), np.array(train_target).ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### AdaBoostClf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def abclf(data, target):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2,\n",
    "                                                        random_state=seed, stratify=target)\n",
    "    \n",
    "    gbm = AdaBoostClassifier(learning_rate=0.2, algorithm='SAMME.R')\n",
    "\n",
    "    gbm.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = gbm.predict_proba(X_test)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, predictions[:, 1], pos_label=1)\n",
    "    print(\"AUC-ROC on test: %.4g\" % auc(fpr, tpr))\n",
    "    \n",
    "    skf = StratifiedKFold(target, n_folds=5, random_state=seed)\n",
    "    results = cross_val_score(gbm, data, target, cv=skf, scoring='roc_auc')\n",
    "    for ind, i in enumerate(results):\n",
    "        print(\"fold #{:d}: {:f}\".format(ind, i))\n",
    "        \n",
    "    print(\"mean AUC-ROC on cv : %.4f%% (%.4f%%)\" % (results.mean()*100, results.std()*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC on test: 0.9373\n",
      "fold #0: 0.938887\n",
      "fold #1: 0.934504\n",
      "fold #2: 0.931411\n",
      "fold #3: 0.931305\n",
      "fold #4: 0.936230\n",
      "mean AUC-ROC on cv : 93.4467% (0.2898%)\n"
     ]
    }
   ],
   "source": [
    "abclf(np.array(scaled_train), np.array(train_target).ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blending with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensemble(data, target):\n",
    "\n",
    "    np.random.seed(0)  # seed to shuffle the train set\n",
    "    n_folds = 3\n",
    "    verbose = True\n",
    "    shuffle = False\n",
    "\n",
    "    X, y, = np.array(data), np.array(target).ravel()\n",
    "    X, x_test, y, y_test_ = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "    \n",
    "    skf = list(StratifiedKFold(y, n_folds))\n",
    "\n",
    "    \n",
    "    gbm = xgb.XGBClassifier(max_depth=3, learning_rate=0.02, n_estimators=1200,\n",
    "                                            min_child_weight=1, gamma=0.0,subsample=0.8,\n",
    "                                            colsample_bytree=0.8, reg_alpha=0.01, reg_lambda=0.05,\n",
    "                                            scale_pos_weight=1, max_delta_step=9, nthread=15,seed = seed*34)\n",
    "    \n",
    "    gbm2 = xgb.XGBClassifier(max_depth=3, learning_rate=0.02, n_estimators=1200,\n",
    "                                            min_child_weight=1, gamma=0.0,subsample=0.8,\n",
    "                                            colsample_bytree=0.8, reg_alpha=0.01, reg_lambda=0.05,\n",
    "                                            scale_pos_weight=1, max_delta_step=9, nthread=15,seed = seed*61)\n",
    "\n",
    "    clfs = [gbm, gbm2, CalibratedClassifierCV(gbm, method='sigmoid', cv=10), \n",
    "            SGDClassifier(penalty='l1', loss='log', verbose=1, n_jobs=15, n_iter=1000)]\n",
    "    \n",
    "    \n",
    "    print(\"Creating train and test sets for blending.\")\n",
    "\n",
    "    dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n",
    "    dataset_blend_test = np.zeros((x_test.shape[0], len(clfs)))\n",
    "\n",
    "    for j, clf in enumerate(clfs):\n",
    "        print(j, clf)\n",
    "        dataset_blend_test_j = np.zeros((x_test.shape[0], len(skf)))\n",
    "        for i, (train, test) in enumerate(skf):\n",
    "            print(\"Fold\", i)\n",
    "            X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict_proba(X_test)[:, 1]\n",
    "            dataset_blend_train[test, j] = y_pred\n",
    "            dataset_blend_test_j[:, i] = clf.predict(x_test)\n",
    "        dataset_blend_test[:,j] = dataset_blend_test_j.mean(1)\n",
    "    \n",
    "    #print(dataset_blend_train)\n",
    "    print(\"Blending\")\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(dataset_blend_train, y)\n",
    "    \n",
    "    predictions = clf.predict_proba(dataset_blend_test)\n",
    "    print(predictions)\n",
    "    print(predictions.shape)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_, predictions[:,1], pos_label=1)\n",
    "    print(\"AUC : %.4g\" % auc(fpr, tpr))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ensemble(np.array(scaled_train), train_target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ensemble(np.array(get_feature_hash(scaled_train)), train_target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ensemble(np.array(get_features_rf(scaled_train)), train_target.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###  Bagging sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bag_xgb(data, target):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2,\n",
    "                                                        random_state=seed, stratify=target)\n",
    "    \n",
    "    gbm = xgb.XGBClassifier(max_depth=3, learning_rate=0.02, n_estimators=1200,\n",
    "                                            min_child_weight=1, gamma=0.0,subsample=0.8,\n",
    "                                            colsample_bytree=0.8, reg_alpha=0.01, reg_lambda=0.05,\n",
    "                                            scale_pos_weight=1, max_delta_step=9, nthread=15)\n",
    "    \n",
    "    \n",
    "    calibrated_clf = CalibratedClassifierCV(gbm, method='sigmoid', cv=10)\n",
    "    calibrated_clf.fit(X_train, y_train)\n",
    "    y_preds = calibrated_clf.predict_proba(X_test)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_preds[:, 1], pos_label=1)\n",
    "    print(\"AUC-ROC on test: %.4g\" % auc(fpr, tpr))\n",
    "    \n",
    "    skf = StratifiedKFold(target, n_folds=5, random_state=seed)\n",
    "    \n",
    "    results = cross_val_score(calibrated_clf, data, target, cv=skf, scoring='roc_auc')\n",
    "    for ind, i in enumerate(results):\n",
    "        print(\"fold #{:d}: {:f}\".format(ind, i))\n",
    "        \n",
    "    print(\"mean AUC-ROC on cv : %.4f%% (%.4f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bag_xgb(np.array(scaled_train), np.array(train_target).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bag_grid_search(data, target, skf=False):\n",
    "    \n",
    "    if skf:\n",
    "        skf = StratifiedKFold(np.array(target), n_folds=5, random_state=seed)\n",
    "        for train_index, test_index in skf:\n",
    "            X_train, X_test = data[train_index], data[test_index]\n",
    "            y_train, y_test = target[train_index], target[test_index]\n",
    "    else: \n",
    "        X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, stratify=target, random_state=seed)\n",
    "    \n",
    "    bag_params = {   \n",
    "        \"max_samples\":[5, 10, 25],\n",
    "        \"max_features\": [2, 3, 6, 10]\n",
    "        \n",
    "    }\n",
    "    print(\"GridSeachCV proceeding...\")\n",
    "    bag_gs = GridSearchCV(BaggingClassifier(xgb.XGBClassifier(max_depth=3, learning_rate=0.02, n_estimators=1200,\n",
    "                                            min_child_weight=1, gamma=0.0,subsample=0.8,\n",
    "                                            colsample_bytree=0.8, reg_alpha=0.01, reg_lambda=0.05,\n",
    "                                            scale_pos_weight=1, max_delta_step=9, nthread=15), n_estimators=25),\n",
    "                                            bag_params, n_jobs=15, cv=5, verbose=2)\n",
    "    bag_gs.fit(X_train, y_train)\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    \"\"\"skf = StratifiedKFold(target, n_folds=5, random_state=seed)\n",
    "    results = cross_val_score(gbm_gs.best_estimator_, data, target, cv=skf, scoring='roc_auc')\n",
    "    for ind, i in enumerate(results):\n",
    "        print(\"fold #{:d}: {:f}\".format(ind, i))\"\"\"\n",
    "    \n",
    "    for params, mean_score, scores in bag_gs.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() / 2, params))\n",
    "        \n",
    "    print(101*\"=\"+ \"\\nBEST PARAMETERS: \", bag_gs.best_params_, \"\\n\"+101*\"=\"+\"\\n\")\n",
    "    \n",
    "    predictions = bag_gs.best_estimator_.predict_proba(X_test)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, predictions[:, 1], pos_label=1)\n",
    "    print(\"AUC : %.4g\" % auc(fpr, tpr))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_sgd(data, target):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, stratify=target, random_state = 442)\n",
    "    sgd = SGDClassifier(loss='log')\n",
    "    sgd.fit(X_train, y_train)\n",
    "              \n",
    "    pred = sgd.predict_proba(X_test)\n",
    "        \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred[:, 1], pos_label=1)\n",
    "    print(\"AUC-ROC on test : %.4g\" % auc(fpr, tpr))\n",
    "              \n",
    "    skf = StratifiedKFold(target, n_folds=5, random_state=seed)\n",
    "    \n",
    "    results = cross_val_score(sgd, data, target, cv=skf, scoring='roc_auc')\n",
    "    for ind, i in enumerate(results):\n",
    "        print(\"fold #{:d}: {:f}\".format(ind, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC on test : 0.9228\n",
      "fold #0: 0.922054\n",
      "fold #1: 0.922569\n",
      "fold #2: 0.912700\n",
      "fold #3: 0.921973\n",
      "fold #4: 0.922272\n"
     ]
    }
   ],
   "source": [
    "train_sgd(scaled_train, np.array(train_target).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sgd_grid_search(data, target, skf=True):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=seed)\n",
    "    \n",
    "    sgd_params = { \n",
    "       # \"eta0\":[]\n",
    "    }\n",
    "    print(\"GridSeachCV proceeding...\")\n",
    "    sgd_gs = GridSearchCV(SGDClassifier(penalty='l1', alpha=0.00001, loss='log', verbose=1, n_jobs=15, l1_ratio=0.1,\n",
    "                                        random_state=seed, n_iter=200, class_weight='balanced', eta0=0.1), \n",
    "                          sgd_params, n_jobs=15, cv=5, verbose=1)\n",
    "    \n",
    "    sgd_gs.fit(X_train, y_train)\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    for params, mean_score, scores in sgd_gs.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() / 2, params))\n",
    "        \n",
    "    print(101*\"=\"+ \"\\nBEST PARAMETERS: \", sgd_gs.best_params_, \"\\n\"+101*\"=\"+\"\\n\")\n",
    "    \n",
    "    predictions = sgd_gs.best_estimator_.predict_proba(X_test)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, predictions[:, 1], pos_label=1)\n",
    "    print(\"AUC : %.4g\" % auc(fpr, tpr))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "sgd = SGDClassifier(penalty='l1', alpha=0.00001, loss='log', verbose=1, n_jobs=15, l1_ratio=0.1,\n",
    "                                        random_state=seed, n_iter=10, class_weight='balanced', eta0=0.1)\n",
    "\n",
    "sgd.fit(np.array(scaled_train), np.array(train_target).ravel())\n",
    "\n",
    "predictions = sgd.predict_proba(np.array(test_data__))\n",
    "write_to_submission_file(predictions[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights\n",
    "\n",
    "* функции print_corr_matrix, averaging, get_weights реализованы для построения ансамблей моделей\n",
    "* print_corr_matrix - визуализация корреляции между ответами, для того, чтобы  следить за некоррелированность ответов моделей\n",
    "* averaging - простое усреднение ответов\n",
    "* get_weights - оптимизация весов в метрике log_loss\n",
    "---\n",
    "\n",
    "* удачно применить их не вышло.\n",
    "* была модель, которая работала лучше одна - fine tuned xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from scipy.stats import gmean\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_corr_matrix(clfs, X):\n",
    "    res = np.vstack([x.predict_proba(X)[:, 1] for x in clfs])\n",
    "    mat = np.corrcoef(res)\n",
    "    print(\"clf\", end = \"\\t\")\n",
    "    #print(\"\\t\".join([x for x in clfs]))\n",
    "    for i in range(len(clfs)):\n",
    "        print(clfs[i], end = \"\\t\")\n",
    "        print(\"\\t\".join(map(lambda x: str(round(x, 3)), mat[i, :])))\n",
    "    plt.pcolor(mat, cmap = plt.cm.RdBu)\n",
    "    plt.show()\n",
    "\n",
    "def averaging(data, target, test):\n",
    "    \n",
    "    clfs = []\n",
    "    X_train, X_test, y_train, y_test = train_test_split(np.array(data), target, test_size=0.2, \n",
    "                                                        random_state=seed, stratify=target)\n",
    "    \"\"\"for i in [433242, 54332, 7856, 34363, 965]:\n",
    "        \n",
    "        cur_clf = xgb.XGBClassifier(max_depth=3, learning_rate=0.02, n_estimators=1200,\n",
    "                                            min_child_weight=1, gamma=0.0,subsample=0.8,\n",
    "                                            colsample_bytree=0.8, reg_alpha=0.01, reg_lambda=0.05,\n",
    "                                            scale_pos_weight=1, max_delta_step=9, nthread=16, seed=i)\n",
    "        clfs.append(cur_clf)\n",
    "        \"\"\"\n",
    "    #clfs.append(MLPClassifier(alpha=0.0001, hidden_layer_sizes=(300, 200, 400, 400, 500, 200),\n",
    "                         #random_state=seed, activation='logistic', max_iter = 300))\n",
    "    \n",
    "    clfs.append(SVC(C=0.029, kernel='linear', class_weight='balanced', decision_function_shape= 'ovo', probability=True))\n",
    "    \n",
    "    gbm = xgb.XGBClassifier(max_depth=3, learning_rate=0.02, n_estimators=1200,\n",
    "                                            min_child_weight=1, gamma=0.0,subsample=0.8,\n",
    "                                            colsample_bytree=0.8, reg_alpha=0.01, reg_lambda=0.05,\n",
    "                                            scale_pos_weight=1, max_delta_step=9, nthread=15, seed = seed)\n",
    "    \n",
    "    \n",
    "    clfs.append(gbm)\n",
    "    #clfs.append(gbm)\n",
    "    \n",
    "    #clfs.append(CalibratedClassifierCV(gbm, method='sigmoid', cv=15))\n",
    "    \n",
    "    \n",
    "    weights = get_weights(data, target, clfs)\n",
    "     \n",
    "\n",
    "    for clf in clfs:\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "    res = np.vstack([x.predict_proba(X_test)[:, 1] for x in clfs])\n",
    "    y_pred1 = res.mean(axis=0)\n",
    "    print(\"Score (amean):\\t\", roc_auc_score(y_test, y_pred1))\n",
    "    y_pred2 = gmean(res, 0)\n",
    "    print(\"Score (gmean):\\t\", roc_auc_score(y_test, y_pred2))\n",
    "    \n",
    "    res1 = np.vstack([x.predict_proba(test)[:, 1] for x in clfs])\n",
    "    preds = gmean(res1, 0)\n",
    "    \n",
    "    \n",
    "    res = np.vstack([cur_w*cur_clf.predict_proba(X_test)[:, 1] for cur_clf, cur_w in zip(clfs, weights)])\n",
    "    y_pred1 = res.mean(axis=0)\n",
    "    print(\"Score weighted (amean):\\t\", roc_auc_score(y_test, y_pred1))\n",
    "    y_pred2 = gmean(res, 0)\n",
    "    print(\"Score weighted (gmean):\\t\", roc_auc_score(y_test, y_pred2))\n",
    "    \n",
    "    print_corr_matrix(clfs, X_test)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_weights(data, target, models):\n",
    "\n",
    "    ### we need a test set that we didn't train on to find the best weights for combining the classifiers\n",
    "    sss = StratifiedShuffleSplit(target, test_size=0.2, random_state=1234)\n",
    "    for train_index, test_index in sss:\n",
    "        break\n",
    "\n",
    "    train_x, train_y = data.values[train_index], target[train_index]\n",
    "    test_x, test_y = data.values[test_index], target[test_index]\n",
    "\n",
    "    ### building the classifiers\n",
    "    clfs = []\n",
    "    for cur_model in models:\n",
    "        \n",
    "        model = cur_model.fit(train_x, train_y)\n",
    "        print(str(cur_model)[:4]+'LogLoss {score}'.format(score=log_loss(test_y, cur_model.predict_proba(test_x))))\n",
    "        clfs.append(model)\n",
    "\n",
    "\n",
    "    ### finding the optimum weights\n",
    "\n",
    "    predictions = []\n",
    "    pred_df = pd.DataFrame()\n",
    "    for clf in clfs:\n",
    "        preds = clf.predict_proba(test_x)\n",
    "        predictions.append(preds)\n",
    "        fpr, tpr, thresholds = roc_curve(test_y, preds[:, 1], pos_label=1)\n",
    "        print(\"AUC \"+ str(clf)[:4]+ \": %.4g\" % auc(fpr, tpr))\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def log_loss_func(weights):\n",
    "        ''' scipy minimize will pass the weights as a numpy array '''\n",
    "        final_prediction = 0\n",
    "        for weight, prediction in zip(weights, predictions):\n",
    "                final_prediction += weight*prediction\n",
    "\n",
    "        return log_loss(test_y, final_prediction)\n",
    "    \n",
    "    #the algorithms need a starting value, right not we chose 0.5 for all weights\n",
    "    #its better to choose many random starting points and run minimize a few times\n",
    "    starting_values = [0.5]*len(predictions)\n",
    "\n",
    "    #adding constraints  and a different solver as suggested by user 16universe\n",
    "    #https://kaggle2.blob.core.windows.net/forum-message-attachments/75655/2393/otto%20model%20weights.pdf?sv=2012-02-12&se=2015-05-03T21%3A22%3A17Z&sr=b&sp=r&sig=rkeA7EJC%2BiQ%2FJ%2BcMpcA4lYQLFh6ubNqs2XAkGtFsAv0%3D\n",
    "    cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "    #our weights are bound between 0 and 1\n",
    "    bounds = [(0,1)]*len(predictions)\n",
    "\n",
    "    res = minimize(log_loss_func, starting_values, method='SLSQP', bounds=bounds, constraints=cons)\n",
    "\n",
    "    print('Ensamble Score: {best_score}'.format(best_score=res['fun']))\n",
    "    print('Best Weights: {weights}'.format(weights=res['x']))\n",
    "    w = weights=res['x']\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbm = xgb.XGBClassifier(max_depth=3, learning_rate=0.02, n_estimators=1200,\n",
    "                                            min_child_weight=1, gamma=0.0,subsample=0.8,\n",
    "                                            colsample_bytree=0.8, reg_alpha=0.01, reg_lambda=0.05,\n",
    "                                            scale_pos_weight=1, max_delta_step=9, nthread=15)\n",
    "\n",
    "best_models = [CalibratedClassifierCV(gbm, method='sigmoid', cv=10),\n",
    "              SGDClassifier(penalty='l1', loss='log', verbose=1, n_jobs=15, n_iter=1000)]\n",
    "\n",
    "get_weights(scaled_train, train_target.values, best_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = averaging(scaled_train, train_target.values, np.array(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "from sklearn.cross_validation import KFold\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_model(input_dim, output_dim):\n",
    "    model = Sequential()\n",
    "    #model.add(Dropout(0.08, input_shape=(input_dim,)))\n",
    "    model.add(Dense(1024, W_regularizer='l1', activation='sigmoid', input_dim=input_dim))\n",
    "    model.add(BatchNormalization(gamma_regularizer='l1'))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Dense(512, W_regularizer='l1', b_regularizer='l2', input_dim=input_dim))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization(gamma_regularizer='l2'))\n",
    "    model.add(Dropout(0.15))\n",
    "\n",
    "    model.add(Dense(256))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(256, W_regularizer='l1', activation='sigmoid', input_dim=input_dim))\n",
    "    model.add(BatchNormalization(gamma_regularizer='l1'))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Dense(128, W_regularizer='l1', b_regularizer='l2', input_dim=input_dim))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization(gamma_regularizer='l2'))\n",
    "    model.add(Dropout(0.15))\n",
    "\n",
    "    model.add(Dense(128))\n",
    "    model.add(keras.layers.advanced_activations.PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Dense(output_dim, activation='sigmoid'))\n",
    "    model.compile(class_mode = 'binary', loss='binary_crossentropy', optimizer=\"adadelta\")\n",
    "    return model\n",
    "\n",
    "def keras_cv(X, Y):\n",
    "    \n",
    "    input_dim = X.shape[1]\n",
    "    output_dim = 2\n",
    "    nb_folds = 4\n",
    "    kfolds = StratifiedKFold(Y, nb_folds)\n",
    "    #KFold(len(train_target.values), nb_folds)\n",
    "    \n",
    "    \n",
    "    #kfolds = StratifiedShuffleSplit(Y,nb test_size=0.2, random_state=1234)\n",
    "    av_roc = 0.\n",
    "    f = 0\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(Y)\n",
    "\n",
    "    Y = np_utils.to_categorical(y)\n",
    "    \n",
    "    for train, valid in kfolds:\n",
    "        print('---'*20)\n",
    "        print('Fold', f)\n",
    "        print('---'*20)\n",
    "        f += 1\n",
    "        X_train = X[train]\n",
    "        X_valid = X[valid]\n",
    "        Y_train = Y[train]\n",
    "        Y_valid = Y[valid]\n",
    "        y_valid = y[valid]\n",
    "\n",
    "        print(\"Building model...\")\n",
    "        model = build_model(input_dim, output_dim)\n",
    "\n",
    "        print(\"Training model...\")\n",
    "\n",
    "        model.fit(X_train, Y_train, nb_epoch=50, batch_size=32, validation_data=(X_valid, Y_valid), verbose=1)\n",
    "        valid_preds = model.predict_proba(X_valid, verbose=0)\n",
    "        valid_preds = valid_preds[:, 1]\n",
    "        roc = roc_auc_score(y_valid, valid_preds)\n",
    "        print(\"ROC:\", roc)\n",
    "        av_roc += roc\n",
    "\n",
    "    print('Average ROC:', av_roc/nb_folds)\n",
    "    \n",
    "def keras_test(data, target):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, \n",
    "                                                        random_state=seed, stratify=target)\n",
    "    \n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "    input_dim = data.shape[1]\n",
    "    output_dim = 2\n",
    "\n",
    "    #y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "\n",
    "    print(\"Building model...\")\n",
    "    model = build_model(input_dim, output_dim)\n",
    "\n",
    "    print(\"Training model...\")\n",
    "    model.fit(X_train, y_train, nb_epoch=150, batch_size=64, verbose=0)\n",
    "    \n",
    "    preds = model.predict_proba(X_test, verbose=1)\n",
    "    print(\"\\nAUC-ROC on test:\", roc_auc_score(y_test, preds[:, 1]))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_model = keras_test(np.array(scaled_train), np.array(train_target).ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наивная попытка Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(scaled_train), train_target.values, test_size=0.2, \n",
    "                                                        random_state=seed, stratify=train_target.values)\n",
    "\n",
    "gbm1 = xgb.XGBClassifier(max_depth=3, learning_rate=0.02, n_estimators=1200,\n",
    "                                            min_child_weight=1, gamma=0.0,subsample=0.8,\n",
    "                                            colsample_bytree=0.8, reg_alpha=0.01, reg_lambda=0.05,\n",
    "                                            scale_pos_weight=1, max_delta_step=9, nthread=15, seed = 34123)\n",
    "\n",
    "gbm2 = xgb.XGBClassifier(max_depth=3, learning_rate=0.02, n_estimators=1200,\n",
    "                                            min_child_weight=1, gamma=0.0,subsample=0.7,\n",
    "                                            colsample_bytree=0.8, reg_alpha=0.01, reg_lambda=0.05,\n",
    "                                            scale_pos_weight=1, max_delta_step=9, nthread=15, seed = 985456)\n",
    "\n",
    "gbm3 = xgb.XGBClassifier(max_depth=3, learning_rate=0.02, n_estimators=1200,\n",
    "                                            min_child_weight=1, gamma=0.0,subsample=0.6,\n",
    "                                            colsample_bytree=0.8, reg_alpha=0.01, reg_lambda=0.05,\n",
    "                                            scale_pos_weight=1, max_delta_step=9, nthread=15, seed = 256542)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "gbm1.fit(X_train, y_train)\n",
    "gbm2.fit(X_train, y_train)\n",
    "gbm3.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "gbm1_preds = gbm1.predict_proba(X_test)[:, 1]\n",
    "gbm2_preds = gbm2.predict_proba(X_test)[:, 1]\n",
    "gbm3_preds = gbm3.predict_proba(X_test)[:, 1]\n",
    "\n",
    "keras_preds = my_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "for i in [(0.3, 0.2, 0.2, 0.3), (0.1, 0.3, 0.3, 0.3), (0.25, 0.25, 0.25, 0.25)]:\n",
    "    res = i[0]*keras_preds + i[1]*gbm1_preds + i[2]*gbm2_preds +i[3]*gbm3_preds \n",
    "    print(\"Score weighted (gmean):\\t\", roc_auc_score(y_test, res))\n",
    "    print(\"coefs:\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbm = xgb.XGBClassifier(max_depth=3, learning_rate=0.02, n_estimators=1200,\n",
    "                                            min_child_weight=1, gamma=0.0,subsample=0.8,\n",
    "                                            colsample_bytree=0.8, reg_alpha=0.01, reg_lambda=0.05,\n",
    "                                            scale_pos_weight=1, max_delta_step=9, nthread=15)\n",
    "    \n",
    "\n",
    "calibrated_clf = CalibratedClassifierCV(gbm, method='sigmoid', cv=10)\n",
    "calibrated_clf.fit(X_train, y_train)\n",
    "y_preds = calibrated_clf.predict_proba(X_test)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_preds[:, 1], pos_label=1)\n",
    "    print(\"AUC-ROC on test: %.4g\" % auc(fpr, tpr))\n",
    "    \n",
    "    skf = StratifiedKFold(target, n_folds=5, random_state=seed)\n",
    "    \n",
    "    results = cross_val_score(calibrated_clf, data, target, cv=skf, scoring='roc_auc')\n",
    "    for ind, i in enumerate(results):\n",
    "        print(\"fold #{:d}: {:f}\".format(ind, i))\n",
    "        \n",
    "    print(\"mean AUC-ROC on cv : %.4f%% (%.4f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keras_cv(np.array(scaled_train), np.array(train_target).ravel())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
